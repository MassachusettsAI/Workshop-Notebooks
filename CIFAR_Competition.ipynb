{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtKvmZx-WmUu",
        "outputId": "c1137eb9-cc4a-4895-d51f-c5ef7b2a0bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "#@title Insatlling Pyorch\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGU6NwlsXFSt"
      },
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bNfVLRUYqZA",
        "outputId": "beb42638-3d8f-450b-fc34-4523d8dfb361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@title Define Hyperparameters\n",
        "\n",
        "input_size = 3072 # img_size = (32,32) ---> three channels so 3*1024 in total\\\n",
        "num_hidden_layers = 8 #must be greater than 1\n",
        "first_layer_size = 2000\n",
        "min_layer_size = 250\n",
        "hidden_sizes = [] # number of nodes at each hidden layer\n",
        "for i in range(0, num_hidden_layers):\n",
        "  hidden_sizes.append(first_layer_size - ( i*math.floor((first_layer_size - min_layer_size)/(num_hidden_layers - 1)) ))\n",
        "print(\"Dimensions of each hidden layer: \" + str(hidden_sizes))\n",
        "num_classes = 100 # number of output classes discrete range [0,9]\n",
        "num_epochs = 200 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 500 # the size of input data took for one iteration\n",
        "lr = 1e-5 # size of step"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of each hidden layer: [2000, 1750, 1500, 1250, 1000, 750, 500, 250]\n",
            "Dimensions of each hidden layer: [2000, 1750, 1500, 1250, 1000, 750, 500, 250]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCsBCXMwbpH5",
        "outputId": "9b24bb31-ee0b-4975-f264-8902fa847e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@title Downloading CIFAR100 data\n",
        "\n",
        "train_data = dsets.CIFAR100(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.CIFAR100(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfDPBdnYgfGp"
      },
      "source": [
        "#@title Loading the data\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL-YXTvghaz_"
      },
      "source": [
        "#@title Define model class\n",
        "\n",
        "class Net(nn.Module): #inheriting from nn.Module\n",
        "  def __init__(self, input_size, hidden_sizes, num_hidden_layers, num_classes):\n",
        "    super(Net,self).__init__()\n",
        "    self.networkLayers = []\n",
        "    for i in range(0, num_hidden_layers):\n",
        "      if (i == 0):\n",
        "        self.networkLayers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "      else:\n",
        "        self.networkLayers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "    self.networkLayers.append(nn.Linear(hidden_sizes[i], num_classes)) #last layer\n",
        "    self.relu = nn.ReLU()\n",
        "    self.networkLayers = nn.ModuleList(self.networkLayers) #needed because layers are in a list\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = x\n",
        "    for i in range(0, num_hidden_layers):\n",
        "      out = self.networkLayers[i](out)\n",
        "      out = self.relu(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3EPEqbjjfAT"
      },
      "source": [
        "#@title Build the model\n",
        "\n",
        "net = Net(input_size, hidden_sizes, num_hidden_layers, num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLIwvAFj2zH"
      },
      "source": [
        "#@title Define loss-function & optimizer\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss() #this is the logarithm of difference (log loss)\n",
        "optimizer = torch.optim.Adam( net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u75Xa5VckuTH",
        "outputId": "a7ee874b-6e6e-4ca5-ae27-b9be821e60a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Training the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i ,(images,labels) in enumerate(train_gen):\n",
        "    images = Variable(images.view(-1,3072)).cuda()\n",
        "    labels = Variable(labels).cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Step [100/100], Loss: 5.2424\n",
            "Epoch [2/200], Step [100/100], Loss: 5.1289\n",
            "Epoch [3/200], Step [100/100], Loss: 5.1036\n",
            "Epoch [4/200], Step [100/100], Loss: 5.1132\n",
            "Epoch [5/200], Step [100/100], Loss: 5.0764\n",
            "Epoch [6/200], Step [100/100], Loss: 5.1109\n",
            "Epoch [7/200], Step [100/100], Loss: 5.1006\n",
            "Epoch [8/200], Step [100/100], Loss: 4.9141\n",
            "Epoch [9/200], Step [100/100], Loss: 4.8214\n",
            "Epoch [10/200], Step [100/100], Loss: 4.7590\n",
            "Epoch [11/200], Step [100/100], Loss: 4.8112\n",
            "Epoch [12/200], Step [100/100], Loss: 4.7599\n",
            "Epoch [13/200], Step [100/100], Loss: 4.6857\n",
            "Epoch [14/200], Step [100/100], Loss: 4.7082\n",
            "Epoch [15/200], Step [100/100], Loss: 4.7478\n",
            "Epoch [16/200], Step [100/100], Loss: 4.6664\n",
            "Epoch [17/200], Step [100/100], Loss: 4.6929\n",
            "Epoch [18/200], Step [100/100], Loss: 4.7184\n",
            "Epoch [19/200], Step [100/100], Loss: 4.6178\n",
            "Epoch [20/200], Step [100/100], Loss: 4.6676\n",
            "Epoch [21/200], Step [100/100], Loss: 4.7981\n",
            "Epoch [22/200], Step [100/100], Loss: 4.7251\n",
            "Epoch [23/200], Step [100/100], Loss: 4.6199\n",
            "Epoch [24/200], Step [100/100], Loss: 4.6686\n",
            "Epoch [25/200], Step [100/100], Loss: 4.7561\n",
            "Epoch [26/200], Step [100/100], Loss: 4.5815\n",
            "Epoch [27/200], Step [100/100], Loss: 4.5914\n",
            "Epoch [28/200], Step [100/100], Loss: 4.5613\n",
            "Epoch [29/200], Step [100/100], Loss: 4.5927\n",
            "Epoch [30/200], Step [100/100], Loss: 4.6095\n",
            "Epoch [31/200], Step [100/100], Loss: 4.6793\n",
            "Epoch [32/200], Step [100/100], Loss: 4.6887\n",
            "Epoch [33/200], Step [100/100], Loss: 4.6300\n",
            "Epoch [34/200], Step [100/100], Loss: 4.6005\n",
            "Epoch [35/200], Step [100/100], Loss: 4.6553\n",
            "Epoch [36/200], Step [100/100], Loss: 4.7113\n",
            "Epoch [37/200], Step [100/100], Loss: 4.4904\n",
            "Epoch [38/200], Step [100/100], Loss: 4.5362\n",
            "Epoch [39/200], Step [100/100], Loss: 4.5186\n",
            "Epoch [40/200], Step [100/100], Loss: 4.6147\n",
            "Epoch [41/200], Step [100/100], Loss: 4.5378\n",
            "Epoch [42/200], Step [100/100], Loss: 4.5314\n",
            "Epoch [43/200], Step [100/100], Loss: 4.5552\n",
            "Epoch [44/200], Step [100/100], Loss: 4.6345\n",
            "Epoch [45/200], Step [100/100], Loss: 4.4545\n",
            "Epoch [46/200], Step [100/100], Loss: 4.5974\n",
            "Epoch [47/200], Step [100/100], Loss: 4.4502\n",
            "Epoch [48/200], Step [100/100], Loss: 4.4423\n",
            "Epoch [49/200], Step [100/100], Loss: 4.5861\n",
            "Epoch [50/200], Step [100/100], Loss: 4.4693\n",
            "Epoch [51/200], Step [100/100], Loss: 4.4981\n",
            "Epoch [52/200], Step [100/100], Loss: 4.4734\n",
            "Epoch [53/200], Step [100/100], Loss: 4.4615\n",
            "Epoch [54/200], Step [100/100], Loss: 4.4035\n",
            "Epoch [55/200], Step [100/100], Loss: 4.4604\n",
            "Epoch [56/200], Step [100/100], Loss: 4.2969\n",
            "Epoch [57/200], Step [100/100], Loss: 4.3960\n",
            "Epoch [58/200], Step [100/100], Loss: 4.3899\n",
            "Epoch [59/200], Step [100/100], Loss: 4.5772\n",
            "Epoch [60/200], Step [100/100], Loss: 4.4028\n",
            "Epoch [61/200], Step [100/100], Loss: 4.4378\n",
            "Epoch [62/200], Step [100/100], Loss: 4.5060\n",
            "Epoch [63/200], Step [100/100], Loss: 4.4658\n",
            "Epoch [64/200], Step [100/100], Loss: 4.4135\n",
            "Epoch [65/200], Step [100/100], Loss: 4.3211\n",
            "Epoch [66/200], Step [100/100], Loss: 4.4978\n",
            "Epoch [67/200], Step [100/100], Loss: 4.3986\n",
            "Epoch [68/200], Step [100/100], Loss: 4.3246\n",
            "Epoch [69/200], Step [100/100], Loss: 4.4187\n",
            "Epoch [70/200], Step [100/100], Loss: 4.3892\n",
            "Epoch [71/200], Step [100/100], Loss: 4.3048\n",
            "Epoch [72/200], Step [100/100], Loss: 4.4369\n",
            "Epoch [73/200], Step [100/100], Loss: 4.3832\n",
            "Epoch [74/200], Step [100/100], Loss: 4.3805\n",
            "Epoch [75/200], Step [100/100], Loss: 4.4081\n",
            "Epoch [76/200], Step [100/100], Loss: 4.2929\n",
            "Epoch [77/200], Step [100/100], Loss: 4.2291\n",
            "Epoch [78/200], Step [100/100], Loss: 4.3509\n",
            "Epoch [79/200], Step [100/100], Loss: 4.2869\n",
            "Epoch [80/200], Step [100/100], Loss: 4.2638\n",
            "Epoch [81/200], Step [100/100], Loss: 4.3322\n",
            "Epoch [82/200], Step [100/100], Loss: 4.2784\n",
            "Epoch [83/200], Step [100/100], Loss: 4.4511\n",
            "Epoch [84/200], Step [100/100], Loss: 4.2580\n",
            "Epoch [85/200], Step [100/100], Loss: 4.2476\n",
            "Epoch [86/200], Step [100/100], Loss: 4.4298\n",
            "Epoch [87/200], Step [100/100], Loss: 4.2442\n",
            "Epoch [88/200], Step [100/100], Loss: 4.3080\n",
            "Epoch [89/200], Step [100/100], Loss: 4.2649\n",
            "Epoch [90/200], Step [100/100], Loss: 4.3259\n",
            "Epoch [91/200], Step [100/100], Loss: 4.3522\n",
            "Epoch [92/200], Step [100/100], Loss: 4.2228\n",
            "Epoch [93/200], Step [100/100], Loss: 4.3064\n",
            "Epoch [94/200], Step [100/100], Loss: 4.4337\n",
            "Epoch [95/200], Step [100/100], Loss: 4.1651\n",
            "Epoch [96/200], Step [100/100], Loss: 4.4317\n",
            "Epoch [97/200], Step [100/100], Loss: 4.1619\n",
            "Epoch [98/200], Step [100/100], Loss: 4.3659\n",
            "Epoch [99/200], Step [100/100], Loss: 4.2649\n",
            "Epoch [100/200], Step [100/100], Loss: 4.2298\n",
            "Epoch [101/200], Step [100/100], Loss: 4.3709\n",
            "Epoch [102/200], Step [100/100], Loss: 4.3771\n",
            "Epoch [103/200], Step [100/100], Loss: 4.3820\n",
            "Epoch [104/200], Step [100/100], Loss: 4.2818\n",
            "Epoch [105/200], Step [100/100], Loss: 4.2647\n",
            "Epoch [106/200], Step [100/100], Loss: 4.2796\n",
            "Epoch [107/200], Step [100/100], Loss: 4.4025\n",
            "Epoch [108/200], Step [100/100], Loss: 4.3012\n",
            "Epoch [109/200], Step [100/100], Loss: 4.0861\n",
            "Epoch [110/200], Step [100/100], Loss: 4.2455\n",
            "Epoch [111/200], Step [100/100], Loss: 4.3813\n",
            "Epoch [112/200], Step [100/100], Loss: 4.0986\n",
            "Epoch [113/200], Step [100/100], Loss: 4.1906\n",
            "Epoch [114/200], Step [100/100], Loss: 4.1585\n",
            "Epoch [115/200], Step [100/100], Loss: 4.3320\n",
            "Epoch [116/200], Step [100/100], Loss: 4.1502\n",
            "Epoch [117/200], Step [100/100], Loss: 4.2836\n",
            "Epoch [118/200], Step [100/100], Loss: 4.2442\n",
            "Epoch [119/200], Step [100/100], Loss: 4.1718\n",
            "Epoch [120/200], Step [100/100], Loss: 4.1774\n",
            "Epoch [121/200], Step [100/100], Loss: 4.0752\n",
            "Epoch [122/200], Step [100/100], Loss: 4.1612\n",
            "Epoch [123/200], Step [100/100], Loss: 4.1340\n",
            "Epoch [124/200], Step [100/100], Loss: 4.0237\n",
            "Epoch [125/200], Step [100/100], Loss: 4.1218\n",
            "Epoch [126/200], Step [100/100], Loss: 4.2192\n",
            "Epoch [127/200], Step [100/100], Loss: 4.1136\n",
            "Epoch [128/200], Step [100/100], Loss: 3.9841\n",
            "Epoch [129/200], Step [100/100], Loss: 4.1823\n",
            "Epoch [130/200], Step [100/100], Loss: 4.1976\n",
            "Epoch [131/200], Step [100/100], Loss: 4.0838\n",
            "Epoch [132/200], Step [100/100], Loss: 4.2452\n",
            "Epoch [133/200], Step [100/100], Loss: 4.0598\n",
            "Epoch [134/200], Step [100/100], Loss: 4.0942\n",
            "Epoch [135/200], Step [100/100], Loss: 4.1145\n",
            "Epoch [136/200], Step [100/100], Loss: 4.1514\n",
            "Epoch [137/200], Step [100/100], Loss: 4.0918\n",
            "Epoch [138/200], Step [100/100], Loss: 4.1655\n",
            "Epoch [139/200], Step [100/100], Loss: 4.2344\n",
            "Epoch [140/200], Step [100/100], Loss: 4.1378\n",
            "Epoch [141/200], Step [100/100], Loss: 4.1020\n",
            "Epoch [142/200], Step [100/100], Loss: 4.2252\n",
            "Epoch [143/200], Step [100/100], Loss: 3.9867\n",
            "Epoch [144/200], Step [100/100], Loss: 4.1703\n",
            "Epoch [145/200], Step [100/100], Loss: 4.0830\n",
            "Epoch [146/200], Step [100/100], Loss: 4.0155\n",
            "Epoch [147/200], Step [100/100], Loss: 4.0083\n",
            "Epoch [148/200], Step [100/100], Loss: 4.1273\n",
            "Epoch [149/200], Step [100/100], Loss: 3.9673\n",
            "Epoch [150/200], Step [100/100], Loss: 4.1141\n",
            "Epoch [151/200], Step [100/100], Loss: 4.0509\n",
            "Epoch [152/200], Step [100/100], Loss: 4.0494\n",
            "Epoch [153/200], Step [100/100], Loss: 4.0784\n",
            "Epoch [154/200], Step [100/100], Loss: 4.0670\n",
            "Epoch [155/200], Step [100/100], Loss: 4.0870\n",
            "Epoch [156/200], Step [100/100], Loss: 4.1051\n",
            "Epoch [157/200], Step [100/100], Loss: 4.0688\n",
            "Epoch [158/200], Step [100/100], Loss: 4.1818\n",
            "Epoch [159/200], Step [100/100], Loss: 4.1350\n",
            "Epoch [160/200], Step [100/100], Loss: 3.8884\n",
            "Epoch [161/200], Step [100/100], Loss: 4.1118\n",
            "Epoch [162/200], Step [100/100], Loss: 3.9953\n",
            "Epoch [163/200], Step [100/100], Loss: 3.9691\n",
            "Epoch [164/200], Step [100/100], Loss: 4.0502\n",
            "Epoch [165/200], Step [100/100], Loss: 4.0640\n",
            "Epoch [166/200], Step [100/100], Loss: 4.0569\n",
            "Epoch [167/200], Step [100/100], Loss: 3.9030\n",
            "Epoch [168/200], Step [100/100], Loss: 4.0580\n",
            "Epoch [169/200], Step [100/100], Loss: 4.1457\n",
            "Epoch [170/200], Step [100/100], Loss: 4.0068\n",
            "Epoch [171/200], Step [100/100], Loss: 3.9347\n",
            "Epoch [172/200], Step [100/100], Loss: 3.9614\n",
            "Epoch [173/200], Step [100/100], Loss: 3.8874\n",
            "Epoch [174/200], Step [100/100], Loss: 3.9407\n",
            "Epoch [175/200], Step [100/100], Loss: 3.9678\n",
            "Epoch [176/200], Step [100/100], Loss: 4.0766\n",
            "Epoch [177/200], Step [100/100], Loss: 4.0468\n",
            "Epoch [178/200], Step [100/100], Loss: 3.9753\n",
            "Epoch [179/200], Step [100/100], Loss: 4.0772\n",
            "Epoch [180/200], Step [100/100], Loss: 3.9173\n",
            "Epoch [181/200], Step [100/100], Loss: 4.0024\n",
            "Epoch [182/200], Step [100/100], Loss: 3.9898\n",
            "Epoch [183/200], Step [100/100], Loss: 4.0286\n",
            "Epoch [184/200], Step [100/100], Loss: 3.8481\n",
            "Epoch [185/200], Step [100/100], Loss: 3.9662\n",
            "Epoch [186/200], Step [100/100], Loss: 4.0970\n",
            "Epoch [187/200], Step [100/100], Loss: 4.0480\n",
            "Epoch [188/200], Step [100/100], Loss: 3.9575\n",
            "Epoch [189/200], Step [100/100], Loss: 3.9975\n",
            "Epoch [190/200], Step [100/100], Loss: 3.8551\n",
            "Epoch [191/200], Step [100/100], Loss: 3.9783\n",
            "Epoch [192/200], Step [100/100], Loss: 3.9640\n",
            "Epoch [193/200], Step [100/100], Loss: 3.8669\n",
            "Epoch [194/200], Step [100/100], Loss: 3.8718\n",
            "Epoch [195/200], Step [100/100], Loss: 3.7567\n",
            "Epoch [196/200], Step [100/100], Loss: 3.8256\n",
            "Epoch [197/200], Step [100/100], Loss: 3.9528\n",
            "Epoch [198/200], Step [100/100], Loss: 3.8674\n",
            "Epoch [199/200], Step [100/100], Loss: 4.1303\n",
            "Epoch [200/200], Step [100/100], Loss: 3.9812\n",
            "Epoch [1/200], Step [100/100], Loss: 5.2504\n",
            "Epoch [2/200], Step [100/100], Loss: 5.0822\n",
            "Epoch [3/200], Step [100/100], Loss: 5.0247\n",
            "Epoch [4/200], Step [100/100], Loss: 5.1399\n",
            "Epoch [5/200], Step [100/100], Loss: 5.0538\n",
            "Epoch [6/200], Step [100/100], Loss: 5.1043\n",
            "Epoch [7/200], Step [100/100], Loss: 5.0165\n",
            "Epoch [8/200], Step [100/100], Loss: 4.8819\n",
            "Epoch [9/200], Step [100/100], Loss: 4.9345\n",
            "Epoch [10/200], Step [100/100], Loss: 4.8468\n",
            "Epoch [11/200], Step [100/100], Loss: 4.7545\n",
            "Epoch [12/200], Step [100/100], Loss: 4.8398\n",
            "Epoch [13/200], Step [100/100], Loss: 4.7218\n",
            "Epoch [14/200], Step [100/100], Loss: 4.7171\n",
            "Epoch [15/200], Step [100/100], Loss: 4.6001\n",
            "Epoch [16/200], Step [100/100], Loss: 4.7054\n",
            "Epoch [17/200], Step [100/100], Loss: 4.7006\n",
            "Epoch [18/200], Step [100/100], Loss: 4.6626\n",
            "Epoch [19/200], Step [100/100], Loss: 4.6480\n",
            "Epoch [20/200], Step [100/100], Loss: 4.6062\n",
            "Epoch [21/200], Step [100/100], Loss: 4.6629\n",
            "Epoch [22/200], Step [100/100], Loss: 4.5444\n",
            "Epoch [23/200], Step [100/100], Loss: 4.6418\n",
            "Epoch [24/200], Step [100/100], Loss: 4.5434\n",
            "Epoch [25/200], Step [100/100], Loss: 4.6460\n",
            "Epoch [26/200], Step [100/100], Loss: 4.4988\n",
            "Epoch [27/200], Step [100/100], Loss: 4.4658\n",
            "Epoch [28/200], Step [100/100], Loss: 4.5695\n",
            "Epoch [29/200], Step [100/100], Loss: 4.5998\n",
            "Epoch [30/200], Step [100/100], Loss: 4.5133\n",
            "Epoch [31/200], Step [100/100], Loss: 4.5032\n",
            "Epoch [32/200], Step [100/100], Loss: 4.5685\n",
            "Epoch [33/200], Step [100/100], Loss: 4.4963\n",
            "Epoch [34/200], Step [100/100], Loss: 4.4203\n",
            "Epoch [35/200], Step [100/100], Loss: 4.4278\n",
            "Epoch [36/200], Step [100/100], Loss: 4.4936\n",
            "Epoch [37/200], Step [100/100], Loss: 4.4020\n",
            "Epoch [38/200], Step [100/100], Loss: 4.4878\n",
            "Epoch [39/200], Step [100/100], Loss: 4.3871\n",
            "Epoch [40/200], Step [100/100], Loss: 4.4880\n",
            "Epoch [41/200], Step [100/100], Loss: 4.3652\n",
            "Epoch [42/200], Step [100/100], Loss: 4.4054\n",
            "Epoch [43/200], Step [100/100], Loss: 4.4659\n",
            "Epoch [44/200], Step [100/100], Loss: 4.3891\n",
            "Epoch [45/200], Step [100/100], Loss: 4.3980\n",
            "Epoch [46/200], Step [100/100], Loss: 4.5522\n",
            "Epoch [47/200], Step [100/100], Loss: 4.4758\n",
            "Epoch [48/200], Step [100/100], Loss: 4.3611\n",
            "Epoch [49/200], Step [100/100], Loss: 4.3654\n",
            "Epoch [50/200], Step [100/100], Loss: 4.4776\n",
            "Epoch [51/200], Step [100/100], Loss: 4.3104\n",
            "Epoch [52/200], Step [100/100], Loss: 4.3784\n",
            "Epoch [53/200], Step [100/100], Loss: 4.4463\n",
            "Epoch [54/200], Step [100/100], Loss: 4.5033\n",
            "Epoch [55/200], Step [100/100], Loss: 4.3989\n",
            "Epoch [56/200], Step [100/100], Loss: 4.4663\n",
            "Epoch [57/200], Step [100/100], Loss: 4.3568\n",
            "Epoch [58/200], Step [100/100], Loss: 4.4517\n",
            "Epoch [59/200], Step [100/100], Loss: 4.3209\n",
            "Epoch [60/200], Step [100/100], Loss: 4.3064\n",
            "Epoch [61/200], Step [100/100], Loss: 4.3852\n",
            "Epoch [62/200], Step [100/100], Loss: 4.1775\n",
            "Epoch [63/200], Step [100/100], Loss: 4.4534\n",
            "Epoch [64/200], Step [100/100], Loss: 4.3761\n",
            "Epoch [65/200], Step [100/100], Loss: 4.4089\n",
            "Epoch [66/200], Step [100/100], Loss: 4.1970\n",
            "Epoch [67/200], Step [100/100], Loss: 4.4372\n",
            "Epoch [68/200], Step [100/100], Loss: 4.3242\n",
            "Epoch [69/200], Step [100/100], Loss: 4.3773\n",
            "Epoch [70/200], Step [100/100], Loss: 4.4123\n",
            "Epoch [71/200], Step [100/100], Loss: 4.3128\n",
            "Epoch [72/200], Step [100/100], Loss: 4.4166\n",
            "Epoch [73/200], Step [100/100], Loss: 4.2774\n",
            "Epoch [74/200], Step [100/100], Loss: 4.3137\n",
            "Epoch [75/200], Step [100/100], Loss: 4.4320\n",
            "Epoch [76/200], Step [100/100], Loss: 4.3638\n",
            "Epoch [77/200], Step [100/100], Loss: 4.3115\n",
            "Epoch [78/200], Step [100/100], Loss: 4.2540\n",
            "Epoch [79/200], Step [100/100], Loss: 4.3894\n",
            "Epoch [80/200], Step [100/100], Loss: 4.4259\n",
            "Epoch [81/200], Step [100/100], Loss: 4.3962\n",
            "Epoch [82/200], Step [100/100], Loss: 4.2543\n",
            "Epoch [83/200], Step [100/100], Loss: 4.2080\n",
            "Epoch [84/200], Step [100/100], Loss: 4.2079\n",
            "Epoch [85/200], Step [100/100], Loss: 4.3531\n",
            "Epoch [86/200], Step [100/100], Loss: 4.2859\n",
            "Epoch [87/200], Step [100/100], Loss: 4.3464\n",
            "Epoch [88/200], Step [100/100], Loss: 4.1703\n",
            "Epoch [89/200], Step [100/100], Loss: 4.2816\n",
            "Epoch [90/200], Step [100/100], Loss: 4.3131\n",
            "Epoch [91/200], Step [100/100], Loss: 4.4086\n",
            "Epoch [92/200], Step [100/100], Loss: 4.2105\n",
            "Epoch [93/200], Step [100/100], Loss: 4.2265\n",
            "Epoch [94/200], Step [100/100], Loss: 4.2353\n",
            "Epoch [95/200], Step [100/100], Loss: 4.2400\n",
            "Epoch [96/200], Step [100/100], Loss: 4.1062\n",
            "Epoch [97/200], Step [100/100], Loss: 4.2206\n",
            "Epoch [98/200], Step [100/100], Loss: 4.1922\n",
            "Epoch [99/200], Step [100/100], Loss: 4.1266\n",
            "Epoch [100/200], Step [100/100], Loss: 4.1396\n",
            "Epoch [101/200], Step [100/100], Loss: 4.1821\n",
            "Epoch [102/200], Step [100/100], Loss: 4.2215\n",
            "Epoch [103/200], Step [100/100], Loss: 4.0455\n",
            "Epoch [104/200], Step [100/100], Loss: 4.3857\n",
            "Epoch [105/200], Step [100/100], Loss: 4.2077\n",
            "Epoch [106/200], Step [100/100], Loss: 4.1868\n",
            "Epoch [107/200], Step [100/100], Loss: 4.3964\n",
            "Epoch [108/200], Step [100/100], Loss: 4.2701\n",
            "Epoch [109/200], Step [100/100], Loss: 4.2968\n",
            "Epoch [110/200], Step [100/100], Loss: 3.9244\n",
            "Epoch [111/200], Step [100/100], Loss: 4.1767\n",
            "Epoch [112/200], Step [100/100], Loss: 4.0908\n",
            "Epoch [113/200], Step [100/100], Loss: 4.0375\n",
            "Epoch [114/200], Step [100/100], Loss: 4.1461\n",
            "Epoch [115/200], Step [100/100], Loss: 4.1944\n",
            "Epoch [116/200], Step [100/100], Loss: 4.0940\n",
            "Epoch [117/200], Step [100/100], Loss: 4.2657\n",
            "Epoch [118/200], Step [100/100], Loss: 4.2122\n",
            "Epoch [119/200], Step [100/100], Loss: 4.1606\n",
            "Epoch [120/200], Step [100/100], Loss: 4.1423\n",
            "Epoch [121/200], Step [100/100], Loss: 4.2022\n",
            "Epoch [122/200], Step [100/100], Loss: 3.9558\n",
            "Epoch [123/200], Step [100/100], Loss: 4.1381\n",
            "Epoch [124/200], Step [100/100], Loss: 4.0438\n",
            "Epoch [125/200], Step [100/100], Loss: 4.1051\n",
            "Epoch [126/200], Step [100/100], Loss: 4.0105\n",
            "Epoch [127/200], Step [100/100], Loss: 4.1617\n",
            "Epoch [128/200], Step [100/100], Loss: 4.1221\n",
            "Epoch [129/200], Step [100/100], Loss: 4.0421\n",
            "Epoch [130/200], Step [100/100], Loss: 4.2045\n",
            "Epoch [131/200], Step [100/100], Loss: 4.0073\n",
            "Epoch [132/200], Step [100/100], Loss: 4.0366\n",
            "Epoch [133/200], Step [100/100], Loss: 4.1662\n",
            "Epoch [134/200], Step [100/100], Loss: 4.1232\n",
            "Epoch [135/200], Step [100/100], Loss: 4.1258\n",
            "Epoch [136/200], Step [100/100], Loss: 4.1107\n",
            "Epoch [137/200], Step [100/100], Loss: 4.1585\n",
            "Epoch [138/200], Step [100/100], Loss: 4.1851\n",
            "Epoch [139/200], Step [100/100], Loss: 3.9897\n",
            "Epoch [140/200], Step [100/100], Loss: 4.0371\n",
            "Epoch [141/200], Step [100/100], Loss: 4.1371\n",
            "Epoch [142/200], Step [100/100], Loss: 4.1434\n",
            "Epoch [143/200], Step [100/100], Loss: 3.9451\n",
            "Epoch [144/200], Step [100/100], Loss: 4.0840\n",
            "Epoch [145/200], Step [100/100], Loss: 4.0653\n",
            "Epoch [146/200], Step [100/100], Loss: 4.0649\n",
            "Epoch [147/200], Step [100/100], Loss: 4.1529\n",
            "Epoch [148/200], Step [100/100], Loss: 4.1540\n",
            "Epoch [149/200], Step [100/100], Loss: 3.8943\n",
            "Epoch [150/200], Step [100/100], Loss: 4.0208\n",
            "Epoch [151/200], Step [100/100], Loss: 4.0736\n",
            "Epoch [152/200], Step [100/100], Loss: 4.0320\n",
            "Epoch [153/200], Step [100/100], Loss: 4.1352\n",
            "Epoch [154/200], Step [100/100], Loss: 4.1759\n",
            "Epoch [155/200], Step [100/100], Loss: 3.9359\n",
            "Epoch [156/200], Step [100/100], Loss: 3.9605\n",
            "Epoch [157/200], Step [100/100], Loss: 4.0822\n",
            "Epoch [158/200], Step [100/100], Loss: 4.0785\n",
            "Epoch [159/200], Step [100/100], Loss: 4.1129\n",
            "Epoch [160/200], Step [100/100], Loss: 4.0364\n",
            "Epoch [161/200], Step [100/100], Loss: 3.9364\n",
            "Epoch [162/200], Step [100/100], Loss: 3.9448\n",
            "Epoch [163/200], Step [100/100], Loss: 4.0200\n",
            "Epoch [164/200], Step [100/100], Loss: 4.1105\n",
            "Epoch [165/200], Step [100/100], Loss: 3.9111\n",
            "Epoch [166/200], Step [100/100], Loss: 3.9803\n",
            "Epoch [167/200], Step [100/100], Loss: 4.0761\n",
            "Epoch [168/200], Step [100/100], Loss: 4.0770\n",
            "Epoch [169/200], Step [100/100], Loss: 4.0082\n",
            "Epoch [170/200], Step [100/100], Loss: 3.9424\n",
            "Epoch [171/200], Step [100/100], Loss: 3.9143\n",
            "Epoch [172/200], Step [100/100], Loss: 3.9935\n",
            "Epoch [173/200], Step [100/100], Loss: 4.0475\n",
            "Epoch [174/200], Step [100/100], Loss: 4.0245\n",
            "Epoch [175/200], Step [100/100], Loss: 3.8358\n",
            "Epoch [176/200], Step [100/100], Loss: 4.0821\n",
            "Epoch [177/200], Step [100/100], Loss: 3.9148\n",
            "Epoch [178/200], Step [100/100], Loss: 3.8876\n",
            "Epoch [179/200], Step [100/100], Loss: 4.0649\n",
            "Epoch [180/200], Step [100/100], Loss: 4.0314\n",
            "Epoch [181/200], Step [100/100], Loss: 4.0654\n",
            "Epoch [182/200], Step [100/100], Loss: 3.7215\n",
            "Epoch [183/200], Step [100/100], Loss: 3.9534\n",
            "Epoch [184/200], Step [100/100], Loss: 4.0281\n",
            "Epoch [185/200], Step [100/100], Loss: 4.1044\n",
            "Epoch [186/200], Step [100/100], Loss: 3.8847\n",
            "Epoch [187/200], Step [100/100], Loss: 3.9685\n",
            "Epoch [188/200], Step [100/100], Loss: 4.0102\n",
            "Epoch [189/200], Step [100/100], Loss: 3.9619\n",
            "Epoch [190/200], Step [100/100], Loss: 3.9898\n",
            "Epoch [191/200], Step [100/100], Loss: 3.9888\n",
            "Epoch [192/200], Step [100/100], Loss: 3.9508\n",
            "Epoch [193/200], Step [100/100], Loss: 3.9052\n",
            "Epoch [194/200], Step [100/100], Loss: 3.9700\n",
            "Epoch [195/200], Step [100/100], Loss: 3.9635\n",
            "Epoch [196/200], Step [100/100], Loss: 4.0010\n",
            "Epoch [197/200], Step [100/100], Loss: 4.0380\n",
            "Epoch [198/200], Step [100/100], Loss: 3.9759\n",
            "Epoch [199/200], Step [100/100], Loss: 3.8487\n",
            "Epoch [200/200], Step [100/100], Loss: 3.9928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPvMW5jHB9X",
        "outputId": "fd77fcfa-cf73-4c8c-8f59-e67f4aeec010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@title Evaluating the accuracy of the model\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images,labels in test_gen:\n",
        "  images = Variable(images.view(-1,3072)).cuda()\n",
        "  labels = labels.cuda()\n",
        "\n",
        "  output = net(images)\n",
        "  _, predicted = torch.max(output,1)\n",
        "  correct += (predicted == labels).sum()\n",
        "  total += labels.size(0)\n",
        "\n",
        "print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model: 16.000 %\n",
            "Accuracy of the model: 16.000 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}