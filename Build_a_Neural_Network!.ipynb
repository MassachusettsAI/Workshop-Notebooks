{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8baadcca",
      "metadata": {
        "id": "8baadcca"
      },
      "source": [
        "<h1> NUMBER IDENTIFIER PROJECT </h1>\n",
        "<h3> Started February 28, 2022 and completed March 3, 2022 </h3>\n",
        "\n",
        "<p> The goal of this project is to train a neural network that can recognize the digits 0-9 </p>\n",
        "<p> This machine learning project is based on the MNIST dataset, which are 28 pixel by 28 pixel black/white images of digits 0-9. </p>\n",
        "<p> Uses pytorch library to gain access to operations that make programming a neural network easier and matplotlib to display results </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19048c5",
      "metadata": {
        "id": "a19048c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba2d1851",
      "metadata": {
        "id": "ba2d1851"
      },
      "outputs": [],
      "source": [
        "# Creating a dataset and dataloader for the MNIST dataset\n",
        "\n",
        "def load_mnist(batch_size=32, train=True):\n",
        "\n",
        "    to_tensor_transform = torchvision.transforms.ToTensor()\n",
        "\n",
        "\n",
        "    dataset = torchvision.datasets.MNIST(r'C:\\Users\\surya\\UMass Amherst\\CS389\\Homework 3\\data_hw3', train, to_tensor_transform, download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size) #This creates an object that will help organize our data into batches\n",
        "    return dataset, dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009d4354",
      "metadata": {
        "id": "009d4354"
      },
      "outputs": [],
      "source": [
        "def plot_image_and_label(image, label):\n",
        "\n",
        "    '''\n",
        "    Takes in an image and label and shows them using matplotlib\n",
        "    this is used to visualize the data and also the outputs of our network\n",
        "    '''\n",
        "\n",
        "    plt.imshow(image)\n",
        "    if type(label) is not int:\n",
        "        _,predicted = torch.max(label,1)\n",
        "        plt.title(\"Best label = \" + str(predicted.item()) + \", with Score: \" + str(round(label[0][predicted].item() * 100,2)))\n",
        "    else:\n",
        "        plt.title(\"Label = \" + str(label))\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff1c254",
      "metadata": {
        "id": "6ff1c254",
        "outputId": "ade33d7b-d831-4b2d-9d01-ae701047ff8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# A test to make sure the data loaded correctly\n",
        "\n",
        "train_dataset, train_dataloader = load_mnist(batch_size=1, train=True)\n",
        "ex_image, ex_label = train_dataset[random.randint(0,1000)]\n",
        "plot_image_and_label(ex_image.reshape(28,28), ex_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc598db5",
      "metadata": {
        "id": "dc598db5"
      },
      "source": [
        "Pytorch is built off of modules (called ```nn.Module```) which consist of 2 parts: The initialization (defined in ```__init__()``` -- note that this the python convention for initalizing classes) and the forward pass (defined aptly as ```forward()```)\n",
        "\n",
        "The first attempt at making the model will revolve around a single-layer perceptron, which is a single linear layer that takes in a flattened image. It's output will be a row of 10 probability scores, where each score corresponds to the likelihood of the image being eachone of the 10 digits. The highest probability score will be the prediction of the model\n",
        "\n",
        "**Let's see how the single-layer perceptron does!**\n",
        "\n",
        "To build a single layer perceptron, we can use the constructor `nn.Linear(size_of_input,size_of_output)` to create a Pytorch object for a simple linear layer. Try it below!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fada9d60",
      "metadata": {
        "id": "fada9d60"
      },
      "outputs": [],
      "source": [
        "class MyPerceptron(nn.Module):\n",
        "    # Defines the layers used in the neural network along with other variables\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MyPerceptron, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # TODO use nn.Linear(....) to create an attribute of self to store a linear layer\n",
        "        ######################################\n",
        "\n",
        "        ######################################\n",
        "    # Passes the input data x through a series of layers defined in __init__. The variable outputted at the end of this series is the prediction, which is returned\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, self.input_size) # this is how you flatten the image to fit into your linear layer. You don't have to change this line.\n",
        "\n",
        "        # TODO call the linear layer you created above as though it's a function. The input should be x.\n",
        "        # return the output of caling your linear layer\n",
        "        ######################################\n",
        "\n",
        "\n",
        "        ######################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab93508",
      "metadata": {
        "id": "8ab93508",
        "outputId": "9af4cc16-c842-426f-a4cd-31d06b3bde75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "max() received an invalid combination of arguments - got (NoneType, int), but expected one of:\n * (Tensor input, *, Tensor out = None)\n * (Tensor input, Tensor other, *, Tensor out = None)\n * (Tensor input, int dim, bool keepdim = False, *, tuple of Tensors out = None)\n * (Tensor input, name dim, bool keepdim = False, *, tuple of Tensors out = None)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-dd6766f65013>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplot_image_and_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-3468ef507d0f>\u001b[0m in \u001b[0;36mplot_image_and_label\u001b[0;34m(image, label)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best label = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", with Score: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (NoneType, int), but expected one of:\n * (Tensor input, *, Tensor out = None)\n * (Tensor input, Tensor other, *, Tensor out = None)\n * (Tensor input, int dim, bool keepdim = False, *, tuple of Tensors out = None)\n * (Tensor input, name dim, bool keepdim = False, *, tuple of Tensors out = None)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# This takes our randomly initialized perceptron and sees its prediction on a random input from MNIST\n",
        "test_model = MyPerceptron(784, 10)\n",
        "test_output = test_model(ex_image.flatten())\n",
        "\n",
        "plot_image_and_label(ex_image.reshape(28,28), test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cc9d6c",
      "metadata": {
        "id": "66cc9d6c"
      },
      "source": [
        "\n",
        "Amazing! It's not trained yet, but it can make predictions!\n",
        "\n",
        "Let's try making a Multi-Layer Perceptron(MLP)\n",
        "Since perceptrons work with linear operations, combining perceptrons would combine all the linear processes into one - still a linear operation. So this is basically the same as working with one perceptron.\n",
        "\n",
        "The way to utilize the extra capacity of multiple layers is to have non-linear function (activation functions) between each layer in the neural network/perceptron. Having the ability to model nonlinear operations greatly improves what, and modifying more weights can be helpful when trying to find more features that can help identify the digit correctly. It will also help the network with identifying relationships between variables that aren't neccesarily linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3408fbe4",
      "metadata": {
        "id": "3408fbe4"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MyMLP, self).__init__()\n",
        "\n",
        "        self.input_size = input_size # making the input size accessible\n",
        "        # TODO use nn.Linear(....) to create a series of linear layers\n",
        "        # Each sucsessive linear layer takes in as many values as the previous layer outputted (e.g. if the output size of layer 1 is 32, the input size of layer 2 is 32)\n",
        "                # TODO perform the forward pass of you model\n",
        "        # use the modules you initialized above (each should be used)\n",
        "        # You should also include the self.relu between each layer (including conv2d ones)\n",
        "        ######################################\n",
        "\n",
        "\n",
        "\n",
        "        ######################################\n",
        "\n",
        "        self.relu = nn.ReLU() # Nonlinearity between layers, this makes the model able to establish non-linear connections between variables.\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, self.input_size)   # This reshapes the input to work with the batches\n",
        "        #Call each of your layers like functions, each on the result of the last, starting by passing x into the first layer\n",
        "        #between layers, call self.relu(output_of_prev_layer)\n",
        "        #store final result in variable \"out\"\n",
        "        ###########\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72fdf39b",
      "metadata": {
        "id": "72fdf39b"
      },
      "outputs": [],
      "source": [
        "# Shows the prediction of the model without training\n",
        "# Not very good huh? (though theres a small chance it is lol)\n",
        "\n",
        "test_model = MyMLP(784, 10)\n",
        "test_output = test_model(ex_image.flatten()) # Notice how we flatten the 2d image into 1d to use the MLP\n",
        "\n",
        "plot_image_and_label(ex_image.reshape(28,28), test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d36d95d",
      "metadata": {
        "id": "4d36d95d"
      },
      "source": [
        "Still incorrect! The reason this model doesn't yet work as intended is because it hasn't been trained yet. The training will allow the model to learn from inputs and outputs, and improve the weights in each layer. Once the weights stop changing, this means that the model has found it's optimality, and the final pass through can be done to identify the digit correctly and with more confidence.\n",
        "\n",
        "The goal of training is to reduce the error, or \"loss\" of the model's predictions. The lower the loss, the more accurate the model will be. Different loss functions can be used to measure error, and the model will perform differently based on which one is implemented. In this case, `nn.CrossEntropyLoss()` will be used. `nn.CrossEntropyLoss` objects are passed the model's 10 probabilities of each number, as well as the correct answer (as a single integer). Feel free to change the loss function being used if you want to play around.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c560c4a",
      "metadata": {
        "id": "9c560c4a"
      },
      "source": [
        "## The Training Process\n",
        "\n",
        "\n",
        "*   Optimizer: an object from `torch.optim` in charge of changing the weights of your model  (usually `torch.optim.SGD or torch.optim.Adam`)\n",
        "*   Loss function: PyTorch object that accepts the prediction of your model and the label of the correct prediction, and calculates the error of your model's prediction\n",
        "\n",
        "\n",
        "```\n",
        "for each epoch:\n",
        "  for each batch of images and labels that the DataLoader gives us on one passthrough of the dataset:\n",
        "    1. Reset the optimizer\n",
        "    2. Use the model to make a prediction\n",
        "    3. Calculate loss on your model's prediction\n",
        "    4. call .backward() on the result of calculating the loss. This calculates the gradient and informs the optimizer how to change the weights\n",
        "    5. call optimizer.step() to update the weights based on the gradient.\n",
        "\n",
        "```\n",
        "\n",
        "### A Word on Gradient Descent:\n",
        "The training will be done through gradient descent, which updates the weights during the backpropogation step from the model's output/label/loss calculation back through the input, so the model can learn from the next piece of input-label data.\n",
        "\n",
        "While it would be nice to theoretically train until the gradient doesn't change, at this point which we know the loss should be at a minimum, this can take an infinitely long time. The gradient may not change much, but it will usually always change. So we repeat this backpropogation process for a certain number of epochs, which we can control. More epochs = more training = more accurate results, but also takes longer to train.\n",
        "\n",
        "The below code tests gradient descent with our model using CrossEntropyLoss. The gradient should be negative, as this would indicate that our loss function is decreasing, which is what we want when training our neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6668b7d",
      "metadata": {
        "id": "a6668b7d"
      },
      "outputs": [],
      "source": [
        "model = MyMLP(784, 10)      # Create model with input size 784 (28 by 28 images, with a 10-vector long output)\n",
        "lr=0.02                     # Learning rate represents by what factor the weights should update per pass through.\n",
        "\n",
        "## TODO Fill in the loss_function and optimizer below and run this cell to see if they are valid!\n",
        "# Hint: Use CrossEntropyLoss from the nn library and Stochastic Gradient Descent (SGD) from the torch.optim library, passing in your model parameters\n",
        "#############################################\n",
        "\n",
        "\n",
        "#############################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0911b2b2",
      "metadata": {
        "id": "0911b2b2"
      },
      "outputs": [],
      "source": [
        "# training function\n",
        "\n",
        "def training(model, loss_function, optimizer, train_dataloader, n_epochs, update_interval):\n",
        "\n",
        "    '''\n",
        "    Updates the parameters of the given model using the optimizer of choice to\n",
        "    reduce the given loss_function\n",
        "\n",
        "    This will iterate over the dataloader 'n_epochs' times training on each batch of images\n",
        "\n",
        "    To get the gradient (which is stored internally in the model) .backward() from the loss tensor is used, and for it to be applied\n",
        "    .step() is used on the optimizer\n",
        "\n",
        "    In between steps the gradient is zeroed so it isn't accidentally reused for the next iteration\n",
        "    '''\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for n in range(n_epochs):\n",
        "        for i, (image, label) in enumerate(tqdm(iter(train_dataloader))):\n",
        "\n",
        "            '''\n",
        "            Updates the parameters of the given model using the optimizer of choice to\n",
        "            reduce the given loss_function\n",
        "\n",
        "            This will iterate over the dataloader 'n_epochs' times training on each batch of images\n",
        "\n",
        "            To get the gradient (which is stored internally in the model) use .backward() from the loss tensor\n",
        "            and to apply it use .step() on the optimizer\n",
        "\n",
        "            In between steps you need to zero the gradient so it can be recalculated -- use .zero_grad for this\n",
        "            '''\n",
        "            # TODO Complete the training loop using the instructions above\n",
        "            # Hint: the above code essentially does one training step\n",
        "            ##########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ##########################\n",
        "\n",
        "            if i % update_interval == 0:\n",
        "                losses.append(round(loss.item(), 2))\n",
        "\n",
        "    return model, losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29ea618",
      "metadata": {
        "id": "f29ea618"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "lr = 0.10               # The size of the step taken when doing gradient descent - initially 0.02\n",
        "batch_size = 64         # The number of images being trained on at once - initially 128\n",
        "update_interval = 100   # The number of batches trained on before recording loss - initially 100\n",
        "n_epochs = 1            # The number of times we train through the entire dataset\n",
        "\n",
        "train_dataset, train_dataloader = load_mnist(batch_size=batch_size, train=True)\n",
        "\n",
        "model = MyMLP(784, 10)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "trained_model, losses = training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)\n",
        "\n",
        "plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses)\n",
        "plt.title(\"training curve\")\n",
        "plt.xlabel(\"number of images trained on\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6fe23e8",
      "metadata": {
        "id": "e6fe23e8"
      },
      "outputs": [],
      "source": [
        "trained_output = trained_model(ex_image.flatten())\n",
        "\n",
        "plot_image_and_label(ex_image.reshape(28,28), trained_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d477617d",
      "metadata": {
        "id": "d477617d"
      },
      "source": [
        "The number is correctly identified, and the score is high, showing that the model is very confident that its answer is correct! However, to show that the model isn't just fit to this training example, and the rest of the training data, we will have to test it.\n",
        "\n",
        "The testing process is the same as training, except test accuracy and average loss are also calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc5a564",
      "metadata": {
        "id": "ffc5a564"
      },
      "outputs": [],
      "source": [
        "def testing(model, loss_function, test_data):\n",
        "\n",
        "    '''\n",
        "    This function will test the given model on the given test_data\n",
        "    it will return the accuracy and the test loss (given by loss_function)\n",
        "    '''\n",
        "\n",
        "    sum_loss = 0\n",
        "    n_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (image, label) in enumerate(tqdm(iter(test_data))):\n",
        "\n",
        "        pred = model(image)\n",
        "        loss = loss_function(pred, label)\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(pred,1)\n",
        "        n_correct += (predicted == label).sum()\n",
        "        total += label.size(0)\n",
        "\n",
        "    test_acc = round(((n_correct / total).item() * 100), 2)\n",
        "    avg_loss = round(sum_loss / len(test_data), 2)\n",
        "\n",
        "    print(\"test accuracy:\", test_acc)\n",
        "    print(\"test loss:\", avg_loss )\n",
        "\n",
        "    return test_acc, avg_loss\n",
        "\n",
        "\n",
        "print(\"testing the previously trained model on test dataset of MNIST\")\n",
        "test_dataset, test_dataloader = load_mnist(batch_size=10000, train=False)\n",
        "_,_ = testing(trained_model, loss_function, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60236ff",
      "metadata": {
        "id": "b60236ff"
      },
      "outputs": [],
      "source": [
        "def train_and_test(model, loss_function, optimizer, batch_size, update_interval, n_epochs):\n",
        "\n",
        "    _, train_dataloader = load_mnist(batch_size=batch_size, train=True)\n",
        "    trained_model, losses = training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)\n",
        "\n",
        "    _, test_dataloader = load_mnist(batch_size=10000, train=False)\n",
        "    test_acc, test_loss = testing(trained_model, loss_function, test_dataloader)\n",
        "\n",
        "    plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses, color=\"b\", label=\"train loss\")\n",
        "    plt.hlines(test_loss, 0, len(losses) * batch_size * update_interval, color='r', label=\"test loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"training curve\")\n",
        "    plt.xlabel(\"number of images trained on\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.show()\n",
        "\n",
        "    return trained_model, test_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Change* the hyperparameters to get the best results!"
      ],
      "metadata": {
        "id": "ff7njCv8CLAb"
      },
      "id": "ff7njCv8CLAb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf172a17",
      "metadata": {
        "id": "bf172a17"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "#CHANGE THESE 4 NUMBERS AND SEE HOW IT IMPACTS YOUR TRAINING GRAPH\n",
        "########################\n",
        "lr = 0.10               # The size of the step taken when doing gradient descent\n",
        "batch_size = 64        # The number of images being trained on at once\n",
        "update_interval = 100   # The number of batches trained on before recording loss\n",
        "n_epochs = 10            # The number of times we train through the entire dataset\n",
        "#####################\n",
        "\n",
        "model = MyMLP(784, 10)\n",
        "loss_function = nn.CrossEntropyLoss()#Change this if you want\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)#Change this if you want\n",
        "\n",
        "_, _ = train_and_test(model, loss_function, optimizer, batch_size=batch_size, update_interval=update_interval, n_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbfadbc",
      "metadata": {
        "id": "abbfadbc"
      },
      "source": [
        "The testing loss ends up being about the same as the training loss, showing that the model is not overfit - it is accurate with both training and testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "YrU44p3r32lX"
      },
      "id": "YrU44p3r32lX"
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MyCNN, self).__init__()\n",
        "\n",
        "        self.input_size = input_size # making the input size accessible\n",
        "\n",
        "        # TODO initalize your layers here\n",
        "        # that makes up the model -- use nn.Conv2d : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "        # Your model can use multiple conv2d layers, but keep track of the dimensions -- they can be tricky\n",
        "        # You can use more if you'd like, but you need at least 2 conv2d layers\n",
        "        #Try different kernel sizes, numbers of kernels, and strides\n",
        "        #the output number of kernels of one layer must == the input number of kernels of the next!\n",
        "        #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU()       # you only need to define one relu, you can use this one\n",
        "\n",
        "        # You need to find what size input MLP you should use (hint: find the overall size of the output of the convs)\n",
        "        # Hint: How big is the output of your last layer when flattened?\n",
        "        ##############\n",
        "        #UNCOMMENT LINE BELOW AND ADD AN INTEGER\n",
        "        #cnn_out_size = YOUR_NUMBER_HERE\n",
        "        ##############\n",
        "\n",
        "        #################################\n",
        "        self.relu = nn.ReLU() #we gave you your ReLU this time\n",
        "\n",
        "        self.lin1 = nn.Linear(cnn_out_size, output_size)\n",
        "        # Add more Linear layers if you want! (recommended)\n",
        "        #if you do, remember to set a different output size for your first layer!\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 1, 28, 28)   # This reshapes the input to work with the batches\n",
        "\n",
        "\n",
        "        #Call all of your Conv2d layers like functions, with ReLU calls in between\n",
        "        #Remember to store the result in the variable \"out\"\n",
        "        ##########################\n",
        "\n",
        "        ##########################\n",
        "\n",
        "        out = out.flatten()  # This is the flattening that we keep talking about (note that it will still be a batch on outputs)\n",
        "        #Call all of your Linear layers like functions, with ReLU calls in between\n",
        "        #Remember to store the result in the variable \"out\"\n",
        "        ##########################\n",
        "\n",
        "        ##########################\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "qqz3n3Q_31n_"
      },
      "id": "qqz3n3Q_31n_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play around with your CNN!"
      ],
      "metadata": {
        "id": "ViQAbnclCB24"
      },
      "id": "ViQAbnclCB24"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "#CHANGE THESE 4 NUMBERS AND SEE HOW IT IMPACTS YOUR TRAINING GRAPH\n",
        "########################\n",
        "lr = 0.10               # The size of the step taken when doing gradient descent\n",
        "batch_size = 64        # The number of images being trained on at once\n",
        "update_interval = 100   # The number of batches trained on before recording loss\n",
        "n_epochs = 10            # The number of times we train through the entire dataset\n",
        "#####################\n",
        "\n",
        "model = MyCNN(784, 10)\n",
        "loss_function = nn.CrossEntropyLoss()#Change this if you want\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)#Change this if you want\n",
        "\n",
        "_, _ = train_and_test(model, loss_function, optimizer, batch_size=batch_size, update_interval=update_interval, n_epochs=n_epochs)"
      ],
      "metadata": {
        "id": "YobsAVcwCBGO"
      },
      "id": "YobsAVcwCBGO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}